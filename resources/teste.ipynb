{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import requirements\n",
    "importlib.reload(requirements)\n",
    "from requirements import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando frame 1/104...\n",
      "Processando frame 2/104...\n",
      "Processando frame 3/104...\n",
      "Processando frame 4/104...\n",
      "Processando frame 5/104...\n",
      "Processando frame 6/104...\n",
      "Processando frame 7/104...\n",
      "Processando frame 8/104...\n",
      "Processando frame 9/104...\n",
      "Processando frame 10/104...\n",
      "Processando frame 11/104...\n",
      "Processando frame 12/104...\n",
      "Processando frame 13/104...\n",
      "Processando frame 14/104...\n",
      "Processando frame 15/104...\n",
      "Processando frame 16/104...\n",
      "Processando frame 17/104...\n",
      "Processando frame 18/104...\n",
      "Processando frame 19/104...\n",
      "Processando frame 20/104...\n",
      "Processando frame 21/104...\n",
      "Processando frame 22/104...\n",
      "Processando frame 23/104...\n",
      "Processando frame 24/104...\n",
      "Processando frame 25/104...\n",
      "Processando frame 26/104...\n",
      "Processando frame 27/104...\n",
      "Processando frame 28/104...\n",
      "Processando frame 29/104...\n",
      "Processando frame 30/104...\n",
      "Processando frame 31/104...\n",
      "Processando frame 32/104...\n",
      "Processando frame 33/104...\n",
      "Processando frame 34/104...\n",
      "Processando frame 35/104...\n",
      "Processando frame 36/104...\n",
      "Processando frame 37/104...\n",
      "Processando frame 38/104...\n",
      "Processando frame 39/104...\n",
      "Processando frame 40/104...\n",
      "Processando frame 41/104...\n",
      "Processando frame 42/104...\n",
      "Processando frame 43/104...\n",
      "Processando frame 44/104...\n",
      "Processando frame 45/104...\n",
      "Processando frame 46/104...\n",
      "Processando frame 47/104...\n",
      "Processando frame 48/104...\n",
      "Processando frame 49/104...\n",
      "Processando frame 50/104...\n",
      "Processando frame 51/104...\n",
      "Processando frame 52/104...\n",
      "Processando frame 53/104...\n",
      "Processando frame 54/104...\n",
      "Processando frame 55/104...\n",
      "Processando frame 56/104...\n",
      "Processando frame 57/104...\n",
      "Processando frame 58/104...\n",
      "Processando frame 59/104...\n",
      "Processando frame 60/104...\n",
      "Processando frame 61/104...\n",
      "Processando frame 62/104...\n",
      "Processando frame 63/104...\n",
      "Processando frame 64/104...\n",
      "Processando frame 65/104...\n",
      "Processando frame 66/104...\n",
      "Processando frame 67/104...\n",
      "Processando frame 68/104...\n",
      "Processando frame 69/104...\n",
      "Processando frame 70/104...\n",
      "Processando frame 71/104...\n",
      "Processando frame 72/104...\n",
      "Processando frame 73/104...\n",
      "Processando frame 74/104...\n",
      "Processando frame 75/104...\n",
      "Processando frame 76/104...\n",
      "Processando frame 77/104...\n",
      "Processando frame 78/104...\n",
      "Processando frame 79/104...\n",
      "Processando frame 80/104...\n",
      "Processando frame 81/104...\n",
      "Processando frame 82/104...\n",
      "Processando frame 83/104...\n",
      "Processando frame 84/104...\n",
      "Processando frame 85/104...\n",
      "Processando frame 86/104...\n",
      "Processando frame 87/104...\n",
      "Processando frame 88/104...\n",
      "Processando frame 89/104...\n",
      "Processando frame 90/104...\n",
      "Processando frame 91/104...\n",
      "Processando frame 92/104...\n",
      "Processando frame 93/104...\n",
      "Processando frame 94/104...\n",
      "Processando frame 95/104...\n",
      "Processando frame 96/104...\n",
      "Processando frame 97/104...\n",
      "Processando frame 98/104...\n",
      "Processando frame 99/104...\n",
      "Processando frame 100/104...\n",
      "Processando frame 101/104...\n",
      "Processando frame 102/104...\n",
      "Processando frame 103/104...\n",
      "Processando frame 104/104...\n",
      "Vídeo processado e salvo em: OUTPUT/teste.mp4\n"
     ]
    }
   ],
   "source": [
    "def process_video_with_selected_face_landmarks(input_video_path, output_video_path):\n",
    "    # Inicializar o MediaPipe Holistic\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Lista de índices de landmarks faciais a serem mantidos\n",
    "    face_landmark_indices = [10, 67, 297, 54, 284, 162, 389, 234, 454, 132, \n",
    "                              361, 58, 288, 136, 365, 149, 378, 148, 377]\n",
    "\n",
    "    # Abrir o vídeo de entrada\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Erro ao abrir o vídeo: {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Obter informações do vídeo\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Configurar o vídeo de saída\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec para salvar vídeo (MP4)\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Inicializar o modelo Holistic\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        frame_idx = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_idx += 1\n",
    "            print(f\"Processando frame {frame_idx}/{frame_count}...\")\n",
    "\n",
    "            # Criar um frame em branco para desenhar os landmarks\n",
    "            blank_frame = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "            # Converter BGR para RGB (requisito do mediapipe)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Processar o frame para landmarks\n",
    "            results = holistic.process(rgb_frame)\n",
    "\n",
    "            # Desenhar landmarks da pose\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    blank_frame, \n",
    "                    results.pose_landmarks, \n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "            # Desenhar landmarks das mãos\n",
    "            if results.left_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    blank_frame, \n",
    "                    results.left_hand_landmarks, \n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2)\n",
    "                )\n",
    "            if results.right_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    blank_frame, \n",
    "                    results.right_hand_landmarks, \n",
    "                    mp_holistic.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "            # Desenhar apenas landmarks faciais filtrados\n",
    "            if results.face_landmarks:\n",
    "                for idx in face_landmark_indices:\n",
    "                    landmark = results.face_landmarks.landmark[idx]\n",
    "                    x = int(landmark.x * frame_width)\n",
    "                    y = int(landmark.y * frame_height)\n",
    "                    cv2.circle(blank_frame, (x, y), 2, (0, 0, 0), -1)\n",
    "\n",
    "            # Escrever o frame anotado no vídeo de saída\n",
    "            out.write(blank_frame)\n",
    "\n",
    "        # Liberar recursos\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Vídeo processado e salvo em: {output_video_path}\")\n",
    "\n",
    "input_video_path = \"INPUT/VIDEOS/MINDS/A/Acontecer1.mp4\"\n",
    "output_video_path = \"OUTPUT/teste.mp4\"\n",
    "process_video_with_selected_face_landmarks(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDataset(Dataset):\n",
    "    def __init__(self, input, method:str):\n",
    "        self.X, self.y = method(input)\n",
    "        self.shape = tuple(self.X.shape)\n",
    "        self.labels = list(set(np.array(self.y)))\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialRecurrentTransformerModel(nn.Module):\n",
    "    def __init__(self, dataset:Dataset, heads, layers, dim_feedforward, dropout):\n",
    "        super(SpatialRecurrentTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(dataset.shape[2], dataset.shape[2])\n",
    "        self.positional_embedding = nn.Parameter(torch.rand(dataset.shape[1], dataset.shape[2]))\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=dataset.shape[2], nhead=heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=layers)\n",
    "        self.feedforward = nn.Sequential(nn.Linear(dataset.shape[2], dim_feedforward), nn.ReLU(), nn.Dropout(dropout), nn.Linear(dim_feedforward, dataset.shape[2]))\n",
    "        self.landmark_attention = nn.MultiheadAttention(embed_dim=dataset.shape[2], num_heads=heads, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dataset.shape[2])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(dataset.shape[2], len(dataset.labels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_embedding.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        spatial_output, _ = self.landmark_attention(x, x, x)\n",
    "        att = self.feedforward(spatial_output)\n",
    "        x = x + att\n",
    "        x = x.sum(dim=1)\n",
    "        #x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(train_accuracy, val_accuracy, max=0.985, threshold=1e-5):\n",
    "    if train_accuracy[-1] >= max and val_accuracy[-1] >= max:\n",
    "        return True\n",
    "    return np.var(train_accuracy[-5:]) <= threshold\n",
    "\n",
    "def train_model(dataset, train, epochs, batch_size, heads, layers, dim_feedforward, dropout, learning_rate, weight_decay):\n",
    "    train_dataset, val_dataset = random_split(dataset, [int(train * len(dataset)), len(dataset) - int(train * len(dataset))])\n",
    "    train_loader, val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(val_dataset, batch_size=batch_size)\n",
    "    model = SpatialRecurrentTransformerModel(dataset, heads, layers, dim_feedforward, dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    progress_bar = tqdm(total=len(train_loader), dynamic_ncols=True, desc=\"Training\", position=0, leave=False)\n",
    "    train_loss, val_loss, train_accuracy, val_accuracy = [0, 0], [0, 0], [0, 0], [0, 0]\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        progress_bar.reset()\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "        all_train_preds, all_train_labels, all_val_preds, all_val_labels = [], [], [], []\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_train_preds.extend(preds.cpu().numpy())\n",
    "            all_train_labels.extend(batch_labels.cpu().numpy())\n",
    "            sum_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"LOSS(train)\": train_loss[-1], \"LOSS(val)\": val_loss[-1], \"ACC(train)\": train_accuracy[-1], \"ACC(val)\": val_accuracy[-1], \"DIFF\": train_accuracy[-1]-train_accuracy[-2]})\n",
    "            progress_bar.update(1)\n",
    "        sum_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                sum_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(batch_labels.cpu().numpy())\n",
    "        train_loss.append(sum_loss/len(train_loader))\n",
    "        val_loss.append(sum_loss/len(val_loader))\n",
    "        train_accuracy.append(accuracy_score(all_train_labels, all_train_preds))\n",
    "        val_accuracy.append(accuracy_score(all_val_labels, all_val_preds))\n",
    "        if validation(train_accuracy, val_accuracy):\n",
    "            break\n",
    "    return model, train_loss, val_loss, train_accuracy, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 0.8\n",
    "EPOCHS = 20\n",
    "BATCH = 16\n",
    "HEADS = 86\n",
    "LAYERS = 6\n",
    "DFF = 1024\n",
    "DROPOUT = 0.1\n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "\n",
    "dataset = torch.load(\"OUTPUT/TENSORS/inerpolate2h.pt\", weights_only=False)\n",
    "model, train_loss, val_loss, train_accuracy, val_accuracy = train_model(dataset, train=TRAIN, epochs=EPOCHS, batch_size=BATCH, heads=HEADS, layers=LAYERS, dim_feedforward=DFF, dropout=DROPOUT, learning_rate=LR, weight_decay=WD)\n",
    "print(f'TRAIN: {train_accuracy[-1]}, VAL: {val_accuracy[-1]}')\n",
    "graph(train_accuracy, val_accuracy, metric='Acurácia')\n",
    "graph(train_loss, val_loss, metric='Perda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
